# 01. Ausgewählte Prinzipien der Rechnerarchitektur

Rechnerarchitektur = Wissenschaft der Grundlagen zum Entwurf von ComputerHardware

### <ins>Software-Seite:</ins>

* Fülle von Andwendungsfeldern mit unterschiedlichen Anforderungen 
* Software-Entwicklung vergleichsweise langsam 
* hohe Flexibilität 
* Hoher Bedarf an neuen Funktionalitäten 
* Wunsch nach hoher Performance  
* ->insgesamt aber langsame zeitliche Entwicklung mit größerer Heterogenität 

----
Abstraktion: Befehl / Instruction Set Architekture (ISA)

### <ins>Hardware-Seite:</ins>

* zwingend hohe Performance, geringer Preis, Wunsch nach "Einfachkeit" Regularität 
* Technologie soll schnell verfügbar sein 
* -> insgesamt etwas schnellere zeitliche Entwicklung mit großer Homogenität in Technologie

----
--Computer führt Befehle aus, die Daten bewegen oder transformieren

### <ins>ISA = Spezifikation einer Menge von Befehlen, die: </ins>

* eine HArdware korrekt implementieren muss
* eine effizente Kontrolle von "Prozessor", Speicher, IO-Geräten erlauben
* eine gewünschte Zielfunktion optimiert 


-Befehlslisten \
-von C-Progamm in  Assembly \
-> Bsp: int x, y, z, / 
        int res = (x+y)-z \
-> in ASS: add x5, x2, x3 /
           sub x6, x5, x4


### <ins>Ausführung von Befehlen:</ins>

* Fetch instruction 
* Decode instruction 
* Fetch operants
* Compute operations 
* Read memory ggf.
* Write memory 
* Update PC

-> längeres Programm induziert eine geordnete Reihenfolge von Befehlen \
-> sequentielle Abstraktion ist richtig und früher wollte man diese an Software-Ebene weiterreichen

### <ins>Beschleunigung von Befehlsausführung:</ins>
-> alle Teilaufgaben benötigen geliche Zeit t 

1. **Minimiert**   
-> vorher: |----|----|----| ( zwischenbereiche sind **t** ) \
-> nacher: |--|--|--| ( Zwischenbereiche sind **t'** ) \
-> somit: t' < t -> dh.: dh nach Minimierung sind ist bereich |--| kleiner (minimiert), als Bereich |----| 

2. **Instructionlevel Parallelism**  
-> gebe Illusion der serielllen Ausführung an Software, aber Parallelität auf Hardware-Ebene \
-> um Parallelität zu schaffen -> Pipelining 

   1. Pipelining  
        -> ohne Pipeline: $T(n) = n * k * t$ \
        -> mit Pipeline: $T_k(n) = (k + n - 1) * t$ \
        -> Speedup $S = T(n)/T_k(n) = n*k*t/(nt + (k-1) t) = k/(1 + ((k-1)/n)) --(n->unednlich)--> k$ \
        -> k = Tiefe der Pipeline  = #Teilaufgaben

    2. Multiple Issue  
        -> Zeit ---> [Befehl 1] und [Befehl 2] untereinander auf gleicher Spalte | [Befehl 3] und [Befehl 4] drunter untereinander auf gleicher Spalte aber nach rechts verschoben 

   3. Speculative Execution  
        Miniaturisierung: \
        -> die Zeit t für die Ausführung einer Teilaufgabe eines Befehls ist *maßgeblich* verantwortlich für die Ausführungszeit eines (großen) Programms \
        -> Bsp: Sei V Volumen der Sprudelflasche, Befüllen/Entleeren erfolgt jeweils mit Geschwindigkeit 10 l/min \
        -> Wechsel von 0 nach 1: dauert T = (V/10l) min \
        -> Sei V' = (V/alpha) um Faktor alpha > 1 kleiner machen \
        -> Sei T' = (V/ (a * 10l)) min = (1/alpha) * T
        -> FAZIT: Information hat keine natürliche Größe und kann in physikalischer Größe runterskaliert werden 

----
### <ins>Zwischenthema: Camputer/Componenten Ebene (NICHT-Klausurrelevant) </ins>
-> Siehe 01_image_0 

### <ins>Zylinderkondensator (NICHT-Klausurenrelevant)</ins>
-> Siehe 01_image_1

-----
### Fazit:
- je kleiner/komprimierter, desto schneller laufen Kondensatoren (Bsp Zylinderkondensator)

- man versucht Rechner kleiner zumachen, weil Sie dann schneller laufen

-----
#### Funktion an Rechner:
- Rechner als Würfel darstellen, wenn man in Rechner von A nach B möchte, dann is Strecke in kleinen Würfel kürzer als in Großen Würfel
- Siehe 01_image_2

- sei **v** Geschwindigkeit der Informationsübertragung 
- -> maximale clock cycle time >= 3L/v ~ L
- -> maximale clock rate <= v/3L ~ 1/L 

        Realitätscheck: 
                1. ENIAC 1946 ->Volumen: 65m^3  ->CycleTime: 200*10^-6 sec
                2. Apple iMac 1981 ->Volumen: 125*10^-6 m^3  ->CycleTime: 0,4*10^-9 sec


- **FAZIT:** kleineres Volumen(Länge) resultiert in kleineren clock cycle timer und damit schnelleren Ausführungen eines Befehls -> Programmausführung

-----
### Erfolgsstory der Miniaturisierung:
- Jahrzehnte lang stieg die Anzahl der Bauelemente (bei gleichem Platz)

#### <ins>Was passierte ca. 2005?</ins>
1. Elektrische Arbeit bei der Verschiebung einer Ladung Q zwischen zwei Punkten, zwischen denen die Spannung U besteht -> W = Q * U

2. Ladung ist proportional zu Spannung -> Q = C * U

3. **Elektrische Leistung (Arbeit/Zeit)** \
-> P = W/T = W * f , wobei f Frequenz \
-> (1)= Q * U * f -> (2)= C * U^2 * f \
Ziel: Elektrische Leistung konstant über Rechnergenerationen \
-> Es war C ~ l , f ~ 1/l \
Von Generation 1 nach Generation 2 \
-> l_1 --> l_2 = aplha*l_1 mit alpha < 1 \
-> c_1 --> c_2 = alpha*c_1 \
-> f_1 --> f_2 = f_1/alpha \
-> durch Miniaturisierung kann Anzahl der Bauelemente von B_1 auf k * B_1 erhöht werden bei gleicher Chip_Fläche, falls Spannung reduziert wird: U_1 --> U_2 = ß * U_1 mit ß < 1 \
-> P_1 C_1 * U_1^2 * f_1 \
-> P_2 k[alpha * c_1 * (ß*U_1)^2 * f_1/alpha] = k * ß^2 C_1 * U_1^2 * f_1 = k * ß^2 * P_1  ->Ziel: k * ß^2 ~~ 1 \
-> etwa: k=2, ß^2 = 1/2, ß~~ 0,7  (Dennard Scalling) \
-> ca. 2005 zu kleine Transistoren "leaky/auslaufende" Transistoren 

#### <ins>Speicher:</ins>
- Logik: trenne Computing (CPU) von Speicher von Neumann-Architektur: [CPU]-----[MEM]
- Vorteile: 
  - ISA mit load-store 
  - Technologie und Skallierung unabhängig voneinander 
- Nachteil: von Neumann-Flaschenhals


         KLAUSURRELEVANT: 
         - Warum ist alles so? - grob (ohne Formeln)
         - Was ist Dennard-Skalling!
                -> Chip wird kleiner, aber Leistung bleibt glich und maximale Taktrate wird höher
                -> es gibt mehr Bauelemte....

#### <ins>Abstrakte Modelle von Berechnungen:</ins> 
- $Z, x, y \in R, Z \leftarrow Z + x * y$

- Datentransfer:
  - Bewege Z von Memory zur CPU 
  - Bewege x von Memory zur CPU 
  - Bewege y von Memory zur CPU 
- Arithmetische Operation: 
  - Multiplizier x und y 
  - Addiere Z dazu 
- Datentransfer:
  - Bewege von CPU zum Memory

- Sei F = # Operationen, Sei G = # Datenstrucktur
- Laufzeit: $T = \alpha*F + \beta * G$  // $\alpha$ = Zeit für 1 Operation;
 $\beta$ = Zeit für Datentransfer
- falls: $\alpha \approx \beta$, dann reicht auch vereinfachendes Modell 
- $T \approx \alpha*F$ oder $T \approx \beta * G$
- FAZIT: Taktfrequenz und Speicherzugriff verbessern sich gleichzeitig (siehe Folie: Taktfrequenz vs Speicherzugriff)

------
    Problem: siehe 01_image_3
-----

#### <ins>RAM-Modell:</ins>
- Zähle # Operationen und ignoriere Speichertransferkosten 
- W(n) = F(n) = $\Theta(n^3)$

#### <ins>ideales Cache Modell</ins>
- 2 Stufige Speicherhierachie mit schnellem Cachen´und langsamen Hauptspeicher
- es gibt imemrnoch Kosten für Hauptspeicher -> man zieht dann Daten aus dem Hauptspeciher in den Cache
- Cache Size: M Worte 
- Cacheline Size: B Worte 
- Hauptspeciher ist beliebig groß
- Datentrasnfer zwischen Cache und Hauptspeicher immer über Cachelines (dh. Blöcke von B Worten, die im Haputspeicher konsekutiv (hintereinanderstehend) abgespeichert sind)
- "Hochgewachsener" Cache: # Cachlines(= M/B) >> # Worte in Cachelines(B)
  - $M = \Omega(B^2)$
- Prozessor verarbeitet nur Daten aus Cache 
- Falls Prozessor auf ein Wort zugreift, gibt es 2 Situationen
  - 1. **Cache Hit:** Wort gehört zu einem Block, der sich bereits in Cache befindet 
  - 2. **Cache Miss (Blocktransfer):** Der Block, der dieses Wort enthält, wird von Hauptspeicher in den Cache geladen
- im Fall eines "Cache Miss" sei Cache ideal, dh. 
  - Neu geladener Block kann an *beliebige* Stelle im Cache geschrieben werden ("voll assoziativ")
  - Ersetzt wird derjenige Block, der zukünftig am längsten nicht benutzt wird  -> "perfekte temporale Lokalität"

-----
#### Kriterien: 
  - Wert = #Operationen = W(n) = F(n)
  - Cache Complexity = # Cache Misses = Q(n; M,B)

#### Annahme für allg. Multiplikation: 
  - Row-Major-Ordnung von Matrizen 
  - in jeder Zeile von links nach rechts 
  - außerdem n genügend groß 

-----
#### <ins>zu Problem aus 01_image_3</ins>: 
- Zeigt wie viel Blockzugriffe wir haben mit "Algorithm 1.1" aus Vorlesungsfolien
- ***siehe 01_image_4***
























 









